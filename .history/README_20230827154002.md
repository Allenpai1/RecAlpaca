# Beyond GPT-3.5 in Conversational Recommendation Systems: Introducing RecAlpaca, a Fine-tuned Model based on Alpaca-7B

<br />

This is the project code which aim to build a dedicated instruction following dataset that not only facilitates the fine-tuning of a language model for explainable recommendations but also a strong recommendation capabilities.

This repo contains:
- The code to generate the data.
- The code to fine-tuning Alpaca-7B
- The code to evaluate Alpaca-7B

## Overview

The RecAlpaca model is a result of fine-tuning a 7B Alpaca model [1] using a dataset consisting of 13k examples of instruction recommendations. These recommendations were generated following the approach described in a research paper referred to as Self-Instruct [2]. The weights of RecAlpaca can be found in HuggingFace Hub with [model card](https://huggingface.co/Allenpai/AlpacaLoraRec).

[1]: Alpaca: Stanford Alpaca: An Instruction-following LLaMA model. Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto. https://github.com/tatsu-lab/stanford_alpaca

[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. https://arxiv.org/abs/2212.10560

## Data Release

The file [`rec_combined_data.json`](./trainingSet/rec_combined_data.json) is a dataset containing 13k instances of instruction-following data used for fine-tuning the RecAlpaca model. This JSON file is structured as a list of dictionaries, where each dictionary contains the following information:

- `instruction`: A string describing the recommendation tasks and their respective domains.
- `input`: A string representing the input, which can either be the highly rated items of the user or a combination of the user's profile and items they like & dislike.
- `output`: A string indicating the generated recommendation items and the reasons for those recommendations. This output is generated using the `gpt3.5-turbo`.

The file [`ML100kEvaluationRecAlpaca.json`](./testSet/ML100kEvaluationRecAlpaca.json) is a test set comprising 200 randomly selected samples from the MovieLens-100k Dataset. These samples follow the same format as described above, but it contains more informations such as:




![plot](./imgs/Flowchart.jpg) 

I tried the handwritten text recognition models on the open-source library KerasOCR; it turns out that the KeraOCR cannot recognize handwritten text, and only English language recognition models are supported. This is because KerasOCR is primarily designed to recognize printed text. 

Therefore to allow muti-language handwritten text detections, I have trained two-stage object detection Faster-RCNN models to detect Chinese or English handwriting questions text from images. The dataset constructed by myself contains my handwritten text from ChatGPT history questions. The detected text image is cropped, and the language classification results pass into PaddleOCR to load the crossposting text extraction model. After that, the detected texts are used as input to pass into the ChatGPT model through an API connection to return an answer. The program will output intermediate step results and save the final Question&Answer to a pdf file. 

## Experiments Environments
 - OS: Intel(R) Core(TM) i5-8259U@230 GHz Macbook pro 2020
 - Raspberry Pi 4B 4GB OS(64-bit)
 - Platform: Python 3.10.8, pytorch 1.13.1, tensorflow 2.11.1, paddleocr 2.6.1.0, openai 0.27.2, OpenCV 4.5.5
 - GPU:A100-SXM4-80GB hired on [AutoDL](https://www.autodl.com/home)
 - IPhone with [IP Camera](https://github.com/shenyaocn/IP-Camera-Bridge) App

## Camera connection and data creation
 - For ground truth object bounding box creation, I have used [labelImg](https://github.com/heartexlabs/labelImg).
 - To connect the iPhone camera to the laptop [IP camera](https://github.com/shenyaocn/IP-Camera-Bridge) was used.
 
## Install requirements
 - ```pip install -r requirements.txt```
 - The pretrained restnet50 backbone model on [VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) can be donwload on [google drive](https://drive.google.com/drive/folders/1bBdFgyOmAyaZJotF_79pKl8VIXhYggee?usp=sharing) and include in Pytorch\ train\ F-RCNN/model_data for training.
 - The full dataset can be download on [Own dataset](https://drive.google.com/drive/folders/1d7Cq-iJxVMWsWlyYrQ-pGN5UGvLxXnRg?usp=sharing).

## My Faster-RCNN Detection training Results
Many experiments are done on the model experiments; please see [my report](./report/report.md) for more details. I have listed the most interesting ones, where the model trained from scratch is slightly less performed and take long training times than with the backbone resnet50 pre-trained model loaded. The best model achieves a 90.02% AmAP. (Also, try not to train a deep model on the CPU).
![plot](./report/compare.png)

## Usage and run
The code under the folder System code is already defined and ready to use, where the main.py is the system connected with the camera and QA_results.pdf is the returning pdf results; it will get updated once you run.

To run the program you can download my pretrained model on [google drive](https://drive.google.com/drive/folders/1bBdFgyOmAyaZJotF_79pKl8VIXhYggee?usp=sharing) and put it in System\ code/logs folder and in the main.py program you need to generate a personal [ChatGPT-API key](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) :
```
 cd System\ code/
 python main.py
```
When the camera is launched, press the ```space``` keyword to take the photo and ```q``` to quit the system.
The system will return the intermediate results, the results will be similar as following:
 1. For Faster-RCNN text language detection:
 
 ![plot](./Images/combine.png)

 2. PaddleOCR text extraction results:
 ![plot](./Images/paddleChinese.png)
 ![plot](./Images/paddleEnglish.png)

 3. QA results in PDF:
 ![plot](./Images/ChineseDoc.png)
 ![plot](./Images/EnglishDoc.png)

 ## Train you own model
Ensure the dataset is in the correct VOC format and put it into the folder PyTorch train F-RCNN and run ```python train.py``` and you need to change your own model_data/classes.txt. It would be beneficial to understand how the network architecture is written by looking at the source code I have included. The source codes are collected from [faster-rcnn-pytorch](https://github.com/bubbliiiing/faster-rcnn-pytorch) and [tensorflow-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn).
![plot](./Images/FRCNN.png)
